{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aecfabfd-48d7-4a0e-bb72-e2bd21bad73d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "create catalog if not exists unified;\n",
    "create schema if not exists system;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb9f0dc8-7bfb-4177-acf7-2a2fd3cc2552",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "use catalog unified;\n",
    "use schema system;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8a301b9-ecd2-48da-a061-2a48c8a05896",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "drop table if exists system.rule;\n",
    "create table system.rule(\n",
    "    `id`            bigint generated always as identity,\n",
    "    `dataset`       string    not null comment 'Name of the dataset to apply the rule',\n",
    "    `column`        string    not null comment 'Name of the column to apply the rule',\n",
    "    `data_type`     string    not null comment 'Data type of the column to apply the rule',\n",
    "    `description`   string    not null comment 'A human readable description of rule and why it''s being applied',\n",
    "    `class`         string    not null comment '''validation'' that evaluates to true or false or ''cleanse'' transforms a value to another value',\n",
    "    `type`          string    not null comment 'Expression, replacement, reference set',\n",
    "    `function`      string    not null comment 'The function to apply to the column',\n",
    "    `parameters`    string             comment 'Arguments to pass into the function, supports using {column} and {relacement} variables',\n",
    "    `replacement`   string             comment 'A replacement value',\n",
    "    `cast_as`       string    not null comment 'Cast the cleanse transform to a specific type',\n",
    "    `order`         string    not null comment 'The order in wich to apply the rule for the table and column',\n",
    "    `created_by`    string    not null comment 'The user who created the rule',\n",
    "    `created_at`    timestamp not null comment 'The time the rule was created'\n",
    ");\n",
    "insert into system.rule (     \n",
    "`dataset`    ,   \n",
    "`column`     ,\n",
    "`data_type`  ,\n",
    "`description`,   \n",
    "`class`      ,   \n",
    "`type`       ,   \n",
    "`function`   ,   \n",
    "`parameters` ,   \n",
    "`replacement`,   \n",
    "`cast_as`    ,   \n",
    "`order`      ,   \n",
    "`created_by` ,   \n",
    "`created_at`    \n",
    ")\n",
    "values\n",
    "('test', 'value', 'string', 'replace all whitespace', 'cleanse', 'function', 'regexp_replace', '{column}, \\'\\\\\\\\s+\\', \\'{replacement}\\'', ' ', 'string', 0, current_user(), now()), \n",
    "('test', 'value', 'string', 'trim whitespace', 'cleanse', 'function', 'trim', '{column}', null, 'string', 1, current_user(), now() ),\n",
    "('test', '*', 'string', 'trim whitespace', 'cleanse', 'function', 'trim', '{column}', null, 'string', 2, current_user(), now() );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4211dd8e-408b-48a8-9c84-995e2a4d19f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "create schema if not exists `test`;\n",
    "drop table if exists `test`.`rule`;\n",
    "create table if not exists `test`.`rule`(\n",
    "  `id`  bigint generated always as identity,\n",
    "  `value` string,\n",
    "  `expected_value` string,\n",
    "  `expected_type` string\n",
    ");\n",
    "\n",
    "insert into `test`.`rule` (`value`, `expected_value`, `expected_type`)\n",
    "values \n",
    "('           stuff  stuff    stuff \\n stuff', 'stuff stuff stuff stuff', 'string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dda192eb-585f-4132-8deb-19f2a8598cab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as fn\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "def cleanse_rule(rule:dict):\n",
    "  variables = ['column', 'replacement']\n",
    "  parameters = rule.get(\"parameters\", \"\") \n",
    "  parameters = \"\" if parameters is None else parameters\n",
    "  if parameters:\n",
    "    for v in variables:\n",
    "      var = \"{\" + v + \"}\"\n",
    "      val = rule.get(v, \"\")\n",
    "      val = \"\" if val is None else val\n",
    "      parameters = parameters.replace(var, val)\n",
    "    parameters = f\"({parameters})\"\n",
    "  else:\n",
    "    parameters = \"()\"\n",
    "\n",
    "  r = {\n",
    "    \"column\": rule[\"column\"],\n",
    "    \"order\": rule[\"order\"],\n",
    "    \"data_type\": rule[\"data_type\"],\n",
    "    \"function\": f\"{rule['function']}{parameters}\",\n",
    "    \"cast_as\": rule[\"cast_as\"],\n",
    "    \"description\": rule[\"description\"]\n",
    "  }\n",
    "  return r\n",
    "\n",
    "\n",
    "def get_rules(column:str, data_type:str, rules:list):\n",
    "\n",
    "  rules = [\n",
    "    r for r in rules\n",
    "    if r['column'] in [column, \"*\"] and r['data_type'] in [data_type, \"*\"]\n",
    "  ]\n",
    "  rules = sorted(rules, key=lambda d: d['order'])\n",
    "  return rules\n",
    "\n",
    "\n",
    "def apply_cleanse(\n",
    "  df:DataFrame, \n",
    "  dataset:str,\n",
    "  enable_pre_cleansed:bool=False\n",
    "):\n",
    "\n",
    "  df_rules = spark.sql(\"\"\"\n",
    "    select *\n",
    "    from system.rule\n",
    "    where dataset = {dataset}\n",
    "    and class = 'cleanse'\n",
    "    order by `column`, `order`               \n",
    "  \"\"\", dataset=dataset).collect()\n",
    "  \n",
    "  rules = [cleanse_rule(rule.asDict()) for rule in df_rules]\n",
    "\n",
    "  for column in df.columns:\n",
    "\n",
    "    data_type = df.schema[column].dataType.typeName()\n",
    "    column_rules = get_rules(column, data_type, rules)\n",
    "\n",
    "    if enable_pre_cleansed:\n",
    "      df = df.withColumn(f\"_pre_cleansed_{column}\", fn.col(column))\n",
    "\n",
    "    for rule in column_rules:\n",
    "      function = rule['function'].replace(\"*\", column)\n",
    "      function = f\"cast({function} as {rule['cast_as']})\"\n",
    "      print(f\"Applying rule {rule['order']}:{rule['description']} using function:{rule['function']} to dataset column {dataset}.{column} casting as {rule['cast_as']}\")\n",
    "      df = df.withColumn(column, fn.expr(function))\n",
    "\n",
    "  return df\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b22175a-c5cf-4591-9a32-122e35232336",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df = spark.sql(\"\"\"\n",
    "  select *\n",
    "  from test.rule\n",
    "\"\"\")\n",
    "\n",
    "df = apply_cleanse(\n",
    "  df, \n",
    "  dataset='test', \n",
    "  enable_pre_cleansed=True\n",
    ") \n",
    "\n",
    "df = (df\n",
    "  .withColumn(\"succeeded\", fn.expr(\"value == expected_value\"))\n",
    ")\n",
    "\n",
    "df.display()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5684234443868397,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "dq",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
